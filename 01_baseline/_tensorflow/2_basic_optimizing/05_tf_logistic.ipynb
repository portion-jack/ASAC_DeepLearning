{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "x_data = np.array([[1,3],[2,2],[3,1],[4,6],[5,5],[6,4]])\n",
    "y_data = np.array([[0],[0],[0],[1],[1],[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-13 12:31:39.788014: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-12-13 12:31:39.788630: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/deep_learning/lib/python3.9/site-packages/keras/initializers/initializers_v2.py:120: UserWarning: The initializer GlorotUniform is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# xavier, he(최적의 초기화 알고리즘)\n",
    "x = tf.constant(x_data,tf.float32)\n",
    "y = tf.constant(y_data,tf.float32)\n",
    "\n",
    "initX = tf.initializers.GlorotUniform()\n",
    "w = tf.Variable(initX(shape=[2,1]))\n",
    "b = tf.Variable(initX(shape=[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost():\n",
    "    z = tf.matmul(x,w) + b\n",
    "    c_i = tf.nn.sigmoid_cross_entropy_with_logits(labels=y,logits=z)\n",
    "    c = tf.reduce_mean(c_i)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 cost: 2.635027\n",
      "1 cost: 2.1148868\n",
      "2 cost: 1.6198668\n",
      "3 cost: 1.1756954\n",
      "4 cost: 0.8287009\n",
      "5 cost: 0.62842906\n",
      "6 cost: 0.57567036\n",
      "7 cost: 0.61421406\n",
      "8 cost: 0.68565935\n",
      "9 cost: 0.75723284\n",
      "10 cost: 0.81508183\n",
      "11 cost: 0.85454214\n",
      "12 cost: 0.8749816\n",
      "13 cost: 0.87752503\n",
      "14 cost: 0.8640811\n",
      "15 cost: 0.83693004\n",
      "16 cost: 0.7985661\n",
      "17 cost: 0.7516575\n",
      "18 cost: 0.6990649\n",
      "19 cost: 0.6438837\n",
      "20 cost: 0.5894796\n",
      "21 cost: 0.5394721\n",
      "22 cost: 0.49757594\n",
      "23 cost: 0.46716475\n",
      "24 cost: 0.45044827\n",
      "25 cost: 0.44739288\n",
      "26 cost: 0.4549726\n",
      "27 cost: 0.4675355\n",
      "28 cost: 0.47848308\n",
      "29 cost: 0.48244905\n",
      "30 cost: 0.47684252\n",
      "31 cost: 0.4621981\n",
      "32 cost: 0.4414699\n",
      "33 cost: 0.41872567\n",
      "34 cost: 0.3977536\n",
      "35 cost: 0.38105187\n",
      "36 cost: 0.36947832\n",
      "37 cost: 0.36251387\n",
      "38 cost: 0.35883868\n",
      "39 cost: 0.35689986\n",
      "40 cost: 0.35530394\n",
      "41 cost: 0.35301527\n",
      "42 cost: 0.349413\n",
      "43 cost: 0.3442652\n",
      "44 cost: 0.33766288\n",
      "45 cost: 0.32993564\n",
      "46 cost: 0.32156226\n",
      "47 cost: 0.31308007\n",
      "48 cost: 0.30499905\n",
      "49 cost: 0.29772455\n",
      "50 cost: 0.2914978\n",
      "51 cost: 0.286363\n",
      "52 cost: 0.28217155\n",
      "53 cost: 0.278626\n",
      "54 cost: 0.27535713\n",
      "55 cost: 0.27201486\n",
      "56 cost: 0.26834792\n",
      "57 cost: 0.26424962\n",
      "58 cost: 0.25975913\n",
      "59 cost: 0.2550242\n",
      "60 cost: 0.2502417\n",
      "61 cost: 0.24559915\n",
      "62 cost: 0.24123196\n",
      "63 cost: 0.23720431\n",
      "64 cost: 0.23351227\n",
      "65 cost: 0.23010103\n",
      "66 cost: 0.2268887\n",
      "67 cost: 0.22378874\n",
      "68 cost: 0.22072743\n",
      "69 cost: 0.21765411\n",
      "70 cost: 0.2145447\n",
      "71 cost: 0.21140003\n",
      "72 cost: 0.20823988\n",
      "73 cost: 0.20509517\n",
      "74 cost: 0.20200017\n",
      "75 cost: 0.19898519\n",
      "76 cost: 0.19607143\n",
      "77 cost: 0.19326839\n",
      "78 cost: 0.19057387\n",
      "79 cost: 0.18797593\n",
      "80 cost: 0.1854567\n",
      "81 cost: 0.18299651\n",
      "82 cost: 0.18057793\n",
      "83 cost: 0.17818837\n",
      "84 cost: 0.17582154\n",
      "85 cost: 0.17347714\n",
      "86 cost: 0.17115955\n",
      "87 cost: 0.16887575\n",
      "88 cost: 0.16663316\n",
      "89 cost: 0.16443785\n",
      "90 cost: 0.16229352\n",
      "91 cost: 0.1602012\n",
      "92 cost: 0.1581592\n",
      "93 cost: 0.15616427\n",
      "94 cost: 0.15421185\n",
      "95 cost: 0.15229745\n",
      "96 cost: 0.15041724\n",
      "97 cost: 0.14856803\n",
      "98 cost: 0.14674819\n",
      "99 cost: 0.14495699\n",
      "100 cost: 0.14319447\n",
      "101 cost: 0.14146133\n",
      "102 cost: 0.1397585\n",
      "103 cost: 0.1380866\n",
      "104 cost: 0.13644597\n",
      "105 cost: 0.13483666\n",
      "106 cost: 0.13325803\n",
      "107 cost: 0.13170923\n",
      "108 cost: 0.13018897\n",
      "109 cost: 0.128696\n",
      "110 cost: 0.12722903\n",
      "111 cost: 0.12578699\n",
      "112 cost: 0.12436895\n",
      "113 cost: 0.12297422\n",
      "114 cost: 0.12160234\n",
      "115 cost: 0.12025305\n",
      "116 cost: 0.11892612\n",
      "117 cost: 0.117621295\n",
      "118 cost: 0.11633836\n",
      "119 cost: 0.11507715\n",
      "120 cost: 0.113837175\n",
      "121 cost: 0.11261812\n",
      "122 cost: 0.1114194\n",
      "123 cost: 0.11024058\n",
      "124 cost: 0.10908111\n",
      "125 cost: 0.107940435\n",
      "126 cost: 0.10681816\n",
      "127 cost: 0.105713665\n",
      "128 cost: 0.104626715\n",
      "129 cost: 0.10355686\n",
      "130 cost: 0.102503836\n",
      "131 cost: 0.10146724\n",
      "132 cost: 0.1004469\n",
      "133 cost: 0.09944242\n",
      "134 cost: 0.09845367\n",
      "135 cost: 0.09748018\n",
      "136 cost: 0.096521795\n",
      "137 cost: 0.095578134\n",
      "138 cost: 0.09464888\n",
      "139 cost: 0.09373379\n",
      "140 cost: 0.092832506\n",
      "141 cost: 0.091944754\n",
      "142 cost: 0.09107029\n",
      "143 cost: 0.09020877\n",
      "144 cost: 0.08936001\n",
      "145 cost: 0.08852373\n",
      "146 cost: 0.08769967\n",
      "147 cost: 0.08688765\n",
      "148 cost: 0.08608743\n",
      "149 cost: 0.08529877\n",
      "150 cost: 0.084521435\n",
      "151 cost: 0.083755255\n",
      "152 cost: 0.08300005\n",
      "153 cost: 0.08225556\n",
      "154 cost: 0.08152156\n",
      "155 cost: 0.08079798\n",
      "156 cost: 0.08008443\n",
      "157 cost: 0.079380855\n",
      "158 cost: 0.07868706\n",
      "159 cost: 0.07800284\n",
      "160 cost: 0.077327974\n",
      "161 cost: 0.07666235\n",
      "162 cost: 0.07600582\n",
      "163 cost: 0.07535815\n",
      "164 cost: 0.074719205\n",
      "165 cost: 0.07408884\n",
      "166 cost: 0.07346691\n",
      "167 cost: 0.07285327\n",
      "168 cost: 0.072247714\n",
      "169 cost: 0.07165019\n",
      "170 cost: 0.071060464\n",
      "171 cost: 0.0704785\n",
      "172 cost: 0.06990406\n",
      "173 cost: 0.0693371\n",
      "174 cost: 0.06877741\n",
      "175 cost: 0.06822491\n",
      "176 cost: 0.06767947\n",
      "177 cost: 0.067141004\n",
      "178 cost: 0.06660928\n",
      "179 cost: 0.06608434\n",
      "180 cost: 0.06556588\n",
      "181 cost: 0.065053955\n",
      "182 cost: 0.06454842\n",
      "183 cost: 0.06404909\n",
      "184 cost: 0.06355595\n",
      "185 cost: 0.06306888\n",
      "186 cost: 0.06258772\n",
      "187 cost: 0.062112413\n",
      "188 cost: 0.061642908\n",
      "189 cost: 0.06117904\n",
      "190 cost: 0.06072074\n",
      "191 cost: 0.060267936\n",
      "192 cost: 0.059820525\n",
      "193 cost: 0.059378445\n",
      "194 cost: 0.058941513\n",
      "195 cost: 0.058509775\n",
      "196 cost: 0.05808308\n",
      "197 cost: 0.057661347\n",
      "198 cost: 0.05724451\n",
      "199 cost: 0.056832533\n",
      "200 cost: 0.05642528\n",
      "201 cost: 0.056022696\n",
      "202 cost: 0.055624723\n",
      "203 cost: 0.055231255\n",
      "204 cost: 0.054842204\n",
      "205 cost: 0.054457575\n",
      "206 cost: 0.054077305\n",
      "207 cost: 0.053701203\n",
      "208 cost: 0.05332934\n",
      "209 cost: 0.052961584\n",
      "210 cost: 0.05259789\n",
      "211 cost: 0.052238256\n",
      "212 cost: 0.051882535\n",
      "213 cost: 0.051530685\n",
      "214 cost: 0.05118264\n",
      "215 cost: 0.050838392\n",
      "216 cost: 0.05049786\n",
      "217 cost: 0.050160967\n",
      "218 cost: 0.049827732\n",
      "219 cost: 0.049498\n",
      "220 cost: 0.049171753\n",
      "221 cost: 0.048848998\n",
      "222 cost: 0.048529606\n",
      "223 cost: 0.048213582\n",
      "224 cost: 0.04790084\n",
      "225 cost: 0.047591373\n",
      "226 cost: 0.047285166\n",
      "227 cost: 0.04698205\n",
      "228 cost: 0.046682082\n",
      "229 cost: 0.04638519\n",
      "230 cost: 0.046091318\n",
      "231 cost: 0.045800447\n",
      "232 cost: 0.04551257\n",
      "233 cost: 0.045227565\n",
      "234 cost: 0.044945415\n",
      "235 cost: 0.04466608\n",
      "236 cost: 0.044389494\n",
      "237 cost: 0.044115752\n",
      "238 cost: 0.043844678\n",
      "239 cost: 0.04357627\n",
      "240 cost: 0.043310534\n",
      "241 cost: 0.04304738\n",
      "242 cost: 0.042786837\n",
      "243 cost: 0.042528786\n",
      "244 cost: 0.042273246\n",
      "245 cost: 0.042020187\n",
      "246 cost: 0.04176953\n",
      "247 cost: 0.041521326\n",
      "248 cost: 0.04127547\n",
      "249 cost: 0.04103195\n",
      "250 cost: 0.040790755\n",
      "251 cost: 0.040551864\n",
      "252 cost: 0.04031514\n",
      "253 cost: 0.040080708\n",
      "254 cost: 0.03984846\n",
      "255 cost: 0.03961838\n",
      "256 cost: 0.039390378\n",
      "257 cost: 0.039164577\n",
      "258 cost: 0.038940817\n",
      "259 cost: 0.038719095\n",
      "260 cost: 0.03849941\n",
      "261 cost: 0.03828176\n",
      "262 cost: 0.038066085\n",
      "263 cost: 0.03785231\n",
      "264 cost: 0.03764053\n",
      "265 cost: 0.03743065\n",
      "266 cost: 0.0372226\n",
      "267 cost: 0.03701646\n",
      "268 cost: 0.036812156\n",
      "269 cost: 0.036609698\n",
      "270 cost: 0.036408965\n",
      "271 cost: 0.036210034\n",
      "272 cost: 0.03601284\n",
      "273 cost: 0.035817377\n",
      "274 cost: 0.035623595\n",
      "275 cost: 0.035431553\n",
      "276 cost: 0.035241175\n",
      "277 cost: 0.035052434\n",
      "278 cost: 0.034865305\n",
      "279 cost: 0.0346798\n",
      "280 cost: 0.034495868\n",
      "281 cost: 0.034313545\n",
      "282 cost: 0.03413271\n",
      "283 cost: 0.033953473\n",
      "284 cost: 0.033775672\n",
      "285 cost: 0.03359942\n",
      "286 cost: 0.033424646\n",
      "287 cost: 0.03325134\n",
      "288 cost: 0.03307946\n",
      "289 cost: 0.03290902\n",
      "290 cost: 0.03273998\n",
      "291 cost: 0.032572374\n",
      "292 cost: 0.032406084\n",
      "293 cost: 0.032241236\n",
      "294 cost: 0.0320777\n",
      "295 cost: 0.031915486\n",
      "296 cost: 0.031754635\n",
      "297 cost: 0.031595048\n",
      "298 cost: 0.03143677\n",
      "299 cost: 0.031279773\n",
      "300 cost: 0.031124044\n",
      "301 cost: 0.030969556\n",
      "302 cost: 0.030816305\n",
      "303 cost: 0.030664265\n",
      "304 cost: 0.030513465\n",
      "305 cost: 0.030363798\n",
      "306 cost: 0.030215312\n",
      "307 cost: 0.030068038\n",
      "308 cost: 0.029921923\n",
      "309 cost: 0.029776936\n",
      "310 cost: 0.029633118\n",
      "311 cost: 0.029490333\n",
      "312 cost: 0.029348692\n",
      "313 cost: 0.02920821\n",
      "314 cost: 0.029068716\n",
      "315 cost: 0.028930327\n",
      "316 cost: 0.028792981\n",
      "317 cost: 0.028656727\n",
      "318 cost: 0.02852149\n",
      "319 cost: 0.028387269\n",
      "320 cost: 0.028254082\n",
      "321 cost: 0.028121892\n",
      "322 cost: 0.02799069\n",
      "323 cost: 0.027860492\n",
      "324 cost: 0.027731245\n",
      "325 cost: 0.027602952\n",
      "326 cost: 0.027475646\n",
      "327 cost: 0.027349278\n",
      "328 cost: 0.02722386\n",
      "329 cost: 0.027099337\n",
      "330 cost: 0.026975777\n",
      "331 cost: 0.026853045\n",
      "332 cost: 0.026731279\n",
      "333 cost: 0.026610417\n",
      "334 cost: 0.026490387\n",
      "335 cost: 0.026371218\n",
      "336 cost: 0.026252892\n",
      "337 cost: 0.026135474\n",
      "338 cost: 0.026018884\n",
      "339 cost: 0.02590312\n",
      "340 cost: 0.025788214\n",
      "341 cost: 0.025674086\n",
      "342 cost: 0.025560774\n",
      "343 cost: 0.0254483\n",
      "344 cost: 0.025336614\n",
      "345 cost: 0.025225725\n",
      "346 cost: 0.025115564\n",
      "347 cost: 0.025006233\n",
      "348 cost: 0.024897616\n",
      "349 cost: 0.024789771\n",
      "350 cost: 0.024682697\n",
      "351 cost: 0.024576392\n",
      "352 cost: 0.024470765\n",
      "353 cost: 0.024365908\n",
      "354 cost: 0.024261754\n",
      "355 cost: 0.024158314\n",
      "356 cost: 0.024055582\n",
      "357 cost: 0.023953572\n",
      "358 cost: 0.023852255\n",
      "359 cost: 0.023751613\n",
      "360 cost: 0.02365163\n",
      "361 cost: 0.023552379\n",
      "362 cost: 0.023453787\n",
      "363 cost: 0.02335579\n",
      "364 cost: 0.02325852\n",
      "365 cost: 0.023161884\n",
      "366 cost: 0.02306591\n",
      "367 cost: 0.022970568\n",
      "368 cost: 0.022875821\n",
      "369 cost: 0.022781707\n",
      "370 cost: 0.022688255\n",
      "371 cost: 0.02259535\n",
      "372 cost: 0.022503149\n",
      "373 cost: 0.022411514\n",
      "374 cost: 0.022320427\n",
      "375 cost: 0.022230003\n",
      "376 cost: 0.022140143\n",
      "377 cost: 0.022050876\n",
      "378 cost: 0.021962173\n",
      "379 cost: 0.021874022\n",
      "380 cost: 0.021786463\n",
      "381 cost: 0.021699479\n",
      "382 cost: 0.021613035\n",
      "383 cost: 0.021527141\n",
      "384 cost: 0.021441799\n",
      "385 cost: 0.021356992\n",
      "386 cost: 0.021272724\n",
      "387 cost: 0.021189023\n",
      "388 cost: 0.021105826\n",
      "389 cost: 0.02102313\n",
      "390 cost: 0.020940995\n",
      "391 cost: 0.020859372\n",
      "392 cost: 0.020778202\n",
      "393 cost: 0.020697623\n",
      "394 cost: 0.0206175\n",
      "395 cost: 0.02053783\n",
      "396 cost: 0.020458741\n",
      "397 cost: 0.020380063\n",
      "398 cost: 0.020301966\n",
      "399 cost: 0.020224253\n",
      "400 cost: 0.020147063\n",
      "401 cost: 0.020070313\n",
      "402 cost: 0.019994047\n",
      "403 cost: 0.019918261\n",
      "404 cost: 0.019842945\n",
      "405 cost: 0.019768082\n",
      "406 cost: 0.019693647\n",
      "407 cost: 0.019619651\n",
      "408 cost: 0.019546125\n",
      "409 cost: 0.019473039\n",
      "410 cost: 0.019400394\n",
      "411 cost: 0.019328218\n",
      "412 cost: 0.019256426\n",
      "413 cost: 0.01918506\n",
      "414 cost: 0.019114137\n",
      "415 cost: 0.019043628\n",
      "416 cost: 0.018973514\n",
      "417 cost: 0.018903818\n",
      "418 cost: 0.018834531\n",
      "419 cost: 0.018765688\n",
      "420 cost: 0.018697217\n",
      "421 cost: 0.018629145\n",
      "422 cost: 0.018561482\n",
      "423 cost: 0.018494222\n",
      "424 cost: 0.01842732\n",
      "425 cost: 0.01836083\n",
      "426 cost: 0.01829474\n",
      "427 cost: 0.01822901\n",
      "428 cost: 0.01816365\n",
      "429 cost: 0.0180987\n",
      "430 cost: 0.01803406\n",
      "431 cost: 0.017969843\n",
      "432 cost: 0.017905984\n",
      "433 cost: 0.017842455\n",
      "434 cost: 0.017779324\n",
      "435 cost: 0.017716542\n",
      "436 cost: 0.017654113\n",
      "437 cost: 0.017592018\n",
      "438 cost: 0.017530292\n",
      "439 cost: 0.017468914\n",
      "440 cost: 0.01740789\n",
      "441 cost: 0.017347172\n",
      "442 cost: 0.017286798\n",
      "443 cost: 0.017226754\n",
      "444 cost: 0.01716704\n",
      "445 cost: 0.017107671\n",
      "446 cost: 0.017048659\n",
      "447 cost: 0.01698998\n",
      "448 cost: 0.016931575\n",
      "449 cost: 0.0168735\n",
      "450 cost: 0.016815782\n",
      "451 cost: 0.016758313\n",
      "452 cost: 0.016701216\n",
      "453 cost: 0.016644409\n",
      "454 cost: 0.016587943\n",
      "455 cost: 0.016531702\n",
      "456 cost: 0.016475871\n",
      "457 cost: 0.016420262\n",
      "458 cost: 0.016364982\n",
      "459 cost: 0.016310006\n",
      "460 cost: 0.016255278\n",
      "461 cost: 0.016200894\n",
      "462 cost: 0.016146827\n",
      "463 cost: 0.016092978\n",
      "464 cost: 0.01603945\n",
      "465 cost: 0.015986195\n",
      "466 cost: 0.015933229\n",
      "467 cost: 0.015880551\n",
      "468 cost: 0.015828123\n",
      "469 cost: 0.015775986\n",
      "470 cost: 0.015724134\n",
      "471 cost: 0.015672546\n",
      "472 cost: 0.015621248\n",
      "473 cost: 0.015570196\n",
      "474 cost: 0.015519392\n",
      "475 cost: 0.015468879\n",
      "476 cost: 0.015418625\n",
      "477 cost: 0.015368635\n",
      "478 cost: 0.015318891\n",
      "479 cost: 0.015269424\n",
      "480 cost: 0.01522019\n",
      "481 cost: 0.01517126\n",
      "482 cost: 0.0151225235\n",
      "483 cost: 0.015074018\n",
      "484 cost: 0.015025832\n",
      "485 cost: 0.014977865\n",
      "486 cost: 0.014930105\n",
      "487 cost: 0.014882608\n",
      "488 cost: 0.014835371\n",
      "489 cost: 0.014788314\n",
      "490 cost: 0.014741573\n",
      "491 cost: 0.014695066\n",
      "492 cost: 0.01464875\n",
      "493 cost: 0.01460263\n",
      "494 cost: 0.014556813\n",
      "495 cost: 0.0145112155\n",
      "496 cost: 0.014465893\n",
      "497 cost: 0.014420709\n",
      "498 cost: 0.0143757565\n",
      "499 cost: 0.014331056\n",
      "500 cost: 0.014286585\n",
      "501 cost: 0.014242297\n",
      "502 cost: 0.014198255\n",
      "503 cost: 0.014154392\n",
      "504 cost: 0.014110779\n",
      "505 cost: 0.0140673835\n",
      "506 cost: 0.014024196\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m optimizer \u001b[39m=\u001b[39m Adam(\u001b[39m0.1\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1000\u001b[39m):\n\u001b[0;32m----> 3\u001b[0m     optimizer\u001b[39m.\u001b[39;49mminimize(cost,var_list\u001b[39m=\u001b[39;49m[w,b])\n\u001b[1;32m      4\u001b[0m     \u001b[39mprint\u001b[39m(i,\u001b[39m'\u001b[39m\u001b[39mcost:\u001b[39m\u001b[39m'\u001b[39m,cost()\u001b[39m.\u001b[39mnumpy())\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/deep_learning/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/optimizer_v2.py:576\u001b[0m, in \u001b[0;36mOptimizerV2.minimize\u001b[0;34m(self, loss, var_list, grad_loss, name, tape)\u001b[0m\n\u001b[1;32m    545\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mminimize\u001b[39m(\u001b[39mself\u001b[39m, loss, var_list, grad_loss\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, tape\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    546\u001b[0m     \u001b[39m\"\"\"Minimize `loss` by updating `var_list`.\u001b[39;00m\n\u001b[1;32m    547\u001b[0m \n\u001b[1;32m    548\u001b[0m \u001b[39m    This method simply computes gradient using `tf.GradientTape` and calls\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    574\u001b[0m \n\u001b[1;32m    575\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 576\u001b[0m     grads_and_vars \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_compute_gradients(\n\u001b[1;32m    577\u001b[0m         loss, var_list\u001b[39m=\u001b[39;49mvar_list, grad_loss\u001b[39m=\u001b[39;49mgrad_loss, tape\u001b[39m=\u001b[39;49mtape\n\u001b[1;32m    578\u001b[0m     )\n\u001b[1;32m    579\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_gradients(grads_and_vars, name\u001b[39m=\u001b[39mname)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/deep_learning/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/optimizer_v2.py:634\u001b[0m, in \u001b[0;36mOptimizerV2._compute_gradients\u001b[0;34m(self, loss, var_list, grad_loss, tape)\u001b[0m\n\u001b[1;32m    632\u001b[0m var_list \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten(var_list)\n\u001b[1;32m    633\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mname_scope(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/gradients\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 634\u001b[0m     grads_and_vars \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_gradients(\n\u001b[1;32m    635\u001b[0m         tape, loss, var_list, grad_loss\n\u001b[1;32m    636\u001b[0m     )\n\u001b[1;32m    638\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_assert_valid_dtypes(\n\u001b[1;32m    639\u001b[0m     [\n\u001b[1;32m    640\u001b[0m         v\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    643\u001b[0m     ]\n\u001b[1;32m    644\u001b[0m )\n\u001b[1;32m    646\u001b[0m \u001b[39mreturn\u001b[39;00m grads_and_vars\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/deep_learning/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/optimizer_v2.py:510\u001b[0m, in \u001b[0;36mOptimizerV2._get_gradients\u001b[0;34m(self, tape, loss, var_list, grad_loss)\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_gradients\u001b[39m(\u001b[39mself\u001b[39m, tape, loss, var_list, grad_loss\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    509\u001b[0m     \u001b[39m\"\"\"Called in `minimize` to compute gradients from loss.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 510\u001b[0m     grads \u001b[39m=\u001b[39m tape\u001b[39m.\u001b[39;49mgradient(loss, var_list, grad_loss)\n\u001b[1;32m    511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(grads, var_list))\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/deep_learning/lib/python3.9/site-packages/tensorflow/python/eager/backprop.py:1113\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1107\u001b[0m   output_gradients \u001b[39m=\u001b[39m (\n\u001b[1;32m   1108\u001b[0m       composite_tensor_gradient\u001b[39m.\u001b[39mget_flat_tensors_for_gradients(\n\u001b[1;32m   1109\u001b[0m           output_gradients))\n\u001b[1;32m   1110\u001b[0m   output_gradients \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m x \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m ops\u001b[39m.\u001b[39mconvert_to_tensor(x)\n\u001b[1;32m   1111\u001b[0m                       \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m output_gradients]\n\u001b[0;32m-> 1113\u001b[0m flat_grad \u001b[39m=\u001b[39m imperative_grad\u001b[39m.\u001b[39;49mimperative_grad(\n\u001b[1;32m   1114\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tape,\n\u001b[1;32m   1115\u001b[0m     flat_targets,\n\u001b[1;32m   1116\u001b[0m     flat_sources,\n\u001b[1;32m   1117\u001b[0m     output_gradients\u001b[39m=\u001b[39;49moutput_gradients,\n\u001b[1;32m   1118\u001b[0m     sources_raw\u001b[39m=\u001b[39;49mflat_sources_raw,\n\u001b[1;32m   1119\u001b[0m     unconnected_gradients\u001b[39m=\u001b[39;49munconnected_gradients)\n\u001b[1;32m   1121\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_persistent:\n\u001b[1;32m   1122\u001b[0m   \u001b[39m# Keep track of watched variables before setting tape to None\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_watched_variables \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tape\u001b[39m.\u001b[39mwatched_variables()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/deep_learning/lib/python3.9/site-packages/tensorflow/python/eager/imperative_grad.py:67\u001b[0m, in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     65\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mUnknown value for unconnected_gradients: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m unconnected_gradients)\n\u001b[0;32m---> 67\u001b[0m \u001b[39mreturn\u001b[39;00m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_TapeGradient(\n\u001b[1;32m     68\u001b[0m     tape\u001b[39m.\u001b[39;49m_tape,  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m     69\u001b[0m     target,\n\u001b[1;32m     70\u001b[0m     sources,\n\u001b[1;32m     71\u001b[0m     output_gradients,\n\u001b[1;32m     72\u001b[0m     sources_raw,\n\u001b[1;32m     73\u001b[0m     compat\u001b[39m.\u001b[39;49mas_str(unconnected_gradients\u001b[39m.\u001b[39;49mvalue))\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/deep_learning/lib/python3.9/site-packages/tensorflow/python/eager/backprop.py:160\u001b[0m, in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    158\u001b[0m     gradient_name_scope \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m forward_pass_name_scope \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    159\u001b[0m   \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mname_scope(gradient_name_scope):\n\u001b[0;32m--> 160\u001b[0m     \u001b[39mreturn\u001b[39;00m grad_fn(mock_op, \u001b[39m*\u001b[39;49mout_grads)\n\u001b[1;32m    161\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m   \u001b[39mreturn\u001b[39;00m grad_fn(mock_op, \u001b[39m*\u001b[39mout_grads)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/deep_learning/lib/python3.9/site-packages/tensorflow/python/ops/math_grad.py:262\u001b[0m, in \u001b[0;36m_MeanGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    259\u001b[0m   output_shape \u001b[39m=\u001b[39m array_ops\u001b[39m.\u001b[39mshape(op\u001b[39m.\u001b[39moutputs[\u001b[39m0\u001b[39m])\n\u001b[1;32m    260\u001b[0m   factor \u001b[39m=\u001b[39m _safe_shape_div(\n\u001b[1;32m    261\u001b[0m       math_ops\u001b[39m.\u001b[39mreduce_prod(input_shape), math_ops\u001b[39m.\u001b[39mreduce_prod(output_shape))\n\u001b[0;32m--> 262\u001b[0m \u001b[39mreturn\u001b[39;00m math_ops\u001b[39m.\u001b[39;49mtruediv(sum_grad, math_ops\u001b[39m.\u001b[39;49mcast(factor, sum_grad\u001b[39m.\u001b[39;49mdtype)), \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/deep_learning/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/deep_learning/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1176\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1177\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m   1178\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/deep_learning/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py:1590\u001b[0m, in \u001b[0;36mtruediv\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   1559\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mmath.truediv\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtruediv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1560\u001b[0m \u001b[39m@dispatch\u001b[39m\u001b[39m.\u001b[39mregister_binary_elementwise_api\n\u001b[1;32m   1561\u001b[0m \u001b[39m@dispatch\u001b[39m\u001b[39m.\u001b[39madd_dispatch_support\n\u001b[1;32m   1562\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtruediv\u001b[39m(x, y, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   1563\u001b[0m   \u001b[39m\"\"\"Divides x / y elementwise (using Python 3 division operator semantics).\u001b[39;00m\n\u001b[1;32m   1564\u001b[0m \n\u001b[1;32m   1565\u001b[0m \u001b[39m  NOTE: Prefer using the Tensor operator or tf.divide which obey Python\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1588\u001b[0m \u001b[39m    TypeError: If `x` and `y` have different dtypes.\u001b[39;00m\n\u001b[1;32m   1589\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1590\u001b[0m   \u001b[39mreturn\u001b[39;00m _truediv_python3(x, y, name)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/deep_learning/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py:1528\u001b[0m, in \u001b[0;36m_truediv_python3\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   1526\u001b[0m   x \u001b[39m=\u001b[39m cast(x, dtype)\n\u001b[1;32m   1527\u001b[0m   y \u001b[39m=\u001b[39m cast(y, dtype)\n\u001b[0;32m-> 1528\u001b[0m \u001b[39mreturn\u001b[39;00m gen_math_ops\u001b[39m.\u001b[39;49mreal_div(x, y, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/deep_learning/lib/python3.9/site-packages/tensorflow/python/ops/gen_math_ops.py:7871\u001b[0m, in \u001b[0;36mreal_div\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   7869\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[1;32m   7870\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 7871\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[1;32m   7872\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mRealDiv\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, x, y)\n\u001b[1;32m   7873\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[1;32m   7874\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = Adam(0.1)\n",
    "for i in range(1000):\n",
    "    optimizer.minimize(cost,var_list=[w,b])\n",
    "    print(i,'cost:',cost().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hxFn(xdata):\n",
    "    xd = tf.constant(xdata,tf.float32)\n",
    "    z = tf.matmul(xd,w) + b\n",
    "    hx = tf.nn.sigmoid(z)\n",
    "    return (hx.numpy()>0.5) + 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hxFn([[5,5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hxFn([[1,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = hxFn(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pred == y_data).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(pred,y_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5fedde54ce020962bd3c30003bddb8a1c5bd9c5a066c739d1bc484f734442d14"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
