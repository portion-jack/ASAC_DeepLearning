{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.nn import Linear, CrossEntropyLoss, Sequential, Softmax\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "x_data = iris['data']\n",
    "y_data = iris['target']\n",
    "\n",
    "# OHE\n",
    "x=torch.FloatTensor(x_data)\n",
    "y=torch.FloatTensor(np.eye(3)[y_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add_module('nn1', Linear(4,3))\n",
    "model.add_module('soft1',Softmax(dim=1))\n",
    "loss_fn =CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.5708151459693909\n",
      "1 0.5708128213882446\n",
      "2 0.5708104968070984\n",
      "3 0.5708081722259521\n",
      "4 0.5708057880401611\n",
      "5 0.5708035230636597\n",
      "6 0.570801317691803\n",
      "7 0.5707988739013672\n",
      "8 0.5707966089248657\n",
      "9 0.5707942843437195\n",
      "10 0.570792019367218\n",
      "11 0.5707896947860718\n",
      "12 0.5707874298095703\n",
      "13 0.5707851052284241\n",
      "14 0.5707828402519226\n",
      "15 0.5707805156707764\n",
      "16 0.5707782506942749\n",
      "17 0.5707759261131287\n",
      "18 0.5707736015319824\n",
      "19 0.570771336555481\n",
      "20 0.5707690715789795\n",
      "21 0.5707667469978333\n",
      "22 0.5707644820213318\n",
      "23 0.5707621574401855\n",
      "24 0.5707598924636841\n",
      "25 0.5707576274871826\n",
      "26 0.5707553625106812\n",
      "27 0.5707530379295349\n",
      "28 0.5707507133483887\n",
      "29 0.5707484483718872\n",
      "30 0.5707461833953857\n",
      "31 0.570743978023529\n",
      "32 0.5707416534423828\n",
      "33 0.5707394480705261\n",
      "34 0.5707371234893799\n",
      "35 0.5707348585128784\n",
      "36 0.570732593536377\n",
      "37 0.5707302689552307\n",
      "38 0.5707280039787292\n",
      "39 0.5707257390022278\n",
      "40 0.5707234740257263\n",
      "41 0.5707212686538696\n",
      "42 0.5707190632820129\n",
      "43 0.5707166790962219\n",
      "44 0.5707144737243652\n",
      "45 0.5707122087478638\n",
      "46 0.5707099437713623\n",
      "47 0.5707076787948608\n",
      "48 0.5707054138183594\n",
      "49 0.5707031488418579\n",
      "50 0.5707009434700012\n",
      "51 0.5706986784934998\n",
      "52 0.5706964135169983\n",
      "53 0.570694088935852\n",
      "54 0.5706919431686401\n",
      "55 0.5706896781921387\n",
      "56 0.5706875324249268\n",
      "57 0.5706851482391357\n",
      "58 0.5706830024719238\n",
      "59 0.5706806778907776\n",
      "60 0.5706784129142761\n",
      "61 0.5706761479377747\n",
      "62 0.5706740021705627\n",
      "63 0.5706716775894165\n",
      "64 0.5706695318222046\n",
      "65 0.5706673264503479\n",
      "66 0.5706650018692017\n",
      "67 0.5706628561019897\n",
      "68 0.5706605911254883\n",
      "69 0.5706583857536316\n",
      "70 0.5706561207771301\n",
      "71 0.5706539154052734\n",
      "72 0.5706517100334167\n",
      "73 0.5706494450569153\n",
      "74 0.5706472396850586\n",
      "75 0.5706449747085571\n",
      "76 0.5706428289413452\n",
      "77 0.5706405639648438\n",
      "78 0.5706382989883423\n",
      "79 0.5706362128257751\n",
      "80 0.5706339478492737\n",
      "81 0.570631742477417\n",
      "82 0.5706294775009155\n",
      "83 0.5706272125244141\n",
      "84 0.5706250667572021\n",
      "85 0.5706228017807007\n",
      "86 0.570620596408844\n",
      "87 0.5706184506416321\n",
      "88 0.5706162452697754\n",
      "89 0.5706140398979187\n",
      "90 0.570611834526062\n",
      "91 0.5706095695495605\n",
      "92 0.5706074237823486\n",
      "93 0.5706052184104919\n",
      "94 0.5706030130386353\n",
      "95 0.5706008672714233\n",
      "96 0.5705986022949219\n",
      "97 0.5705963969230652\n",
      "98 0.5705942511558533\n",
      "99 0.5705919861793518\n",
      "early stop\n"
     ]
    }
   ],
   "source": [
    "patience = 100\n",
    "min_delta = 0.01\n",
    "hist_cost = []\n",
    "\n",
    "point = 0\n",
    "for epoch in range(2000):\n",
    "    optimizer.zero_grad()\n",
    "    hx = model(x)\n",
    "    c = loss_fn(hx,y)\n",
    "    c.backward()\n",
    "    optimizer.step()\n",
    "    hist_cost.append(c.item())\n",
    "    print(epoch, c.item())\n",
    "    if hist_cost[epoch-1] - hist_cost[epoch] > min_delta:\n",
    "        point = 0\n",
    "    else:\n",
    "        point += 1\n",
    "    if point >= patience:\n",
    "        print('early stop')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 | packaged by conda-forge | (main, Mar 24 2022, 23:25:14) \n[Clang 12.0.1 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5fedde54ce020962bd3c30003bddb8a1c5bd9c5a066c739d1bc484f734442d14"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
